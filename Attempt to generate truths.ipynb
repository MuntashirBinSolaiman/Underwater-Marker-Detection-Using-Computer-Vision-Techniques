{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detectMarkers(imagePath):\n",
    "    # Load the image in color\n",
    "    image = cv2.imread(imagePath)\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply adaptive thresholding to handle varying lighting conditions\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Find contours in the thresholded image\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    markerCenters = []\n",
    "\n",
    "    # Iterate through each contour and check if it's circular (approximate contour to a circle)\n",
    "    for contour in contours:\n",
    "        # Calculate the area of the contour\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area < 100:  # Ignore small contours (adjust this threshold if needed)\n",
    "            continue\n",
    "\n",
    "        # Get the approximate bounding circle for the contour\n",
    "        (x, y), radius = cv2.minEnclosingCircle(contour)\n",
    "\n",
    "        # Check if the contour is approximately circular\n",
    "        if radius > 5:  # Ignore very small circles (adjust as necessary)\n",
    "            markerCenters.append((int(x), int(y)))\n",
    "\n",
    "    return markerCenters\n",
    "\n",
    "# List of image paths\n",
    "imagePaths = [\n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00001.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00002.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00003.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00004.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00005.JPG\"\n",
    "]\n",
    "\n",
    "# Detect markers for all images\n",
    "allCenters = {path: detectMarkers(path) for path in imagePaths}\n",
    "\n",
    "# Plot the detected marker points for each image\n",
    "for imagePath, centers in allCenters.items():\n",
    "    # Load the image in color (BGR format)\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "    # Convert the image to RGB (Matplotlib uses RGB format)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image_rgb)\n",
    "    \n",
    "    # Overlay all detected points\n",
    "    for point in centers:\n",
    "        plt.scatter(point[0], point[1], color='red', s=40, label='Detected Points')\n",
    "\n",
    "    plt.title(f\"Detected Marker Points for {imagePath}\")\n",
    "    plt.axis('off')  # Optional: Remove axis for cleaner display\n",
    "    plt.show()\n",
    "\n",
    "# Print the detected points for each image\n",
    "print(\"Detected marker points:\", allCenters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the next code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detectMarkers(imagePath):\n",
    "    # Load the image\n",
    "    image = cv2.imread(imagePath)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    \n",
    "    # Thresholding: Detect dark objects (black markers)\n",
    "    _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)  # Invert for black markers\n",
    "    \n",
    "    # Morphological operations: Remove noise\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)  # Remove small noise\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)  # Fill small holes\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    markerCenters = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        # Calculate contour area\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area < 100:  # Ignore very small contours\n",
    "            continue\n",
    "\n",
    "        # Get circularity: (4π * Area) / (Perimeter²)\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        if perimeter == 0:\n",
    "            continue  # Avoid division by zero\n",
    "        \n",
    "        circularity = (4 * np.pi * area) / (perimeter ** 2)\n",
    "        \n",
    "        # Accept only circular objects (close to 1.0)\n",
    "        if 0.7 < circularity <= 1.2:  # Adjust range if needed\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                markerCenters.append((cx, cy))\n",
    "    \n",
    "    return markerCenters\n",
    "\n",
    "# List of image paths\n",
    "imagePaths = [\n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00001.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00002.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00003.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00004.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00005.JPG\"\n",
    "]\n",
    "\n",
    "# Detect markers for all images\n",
    "allCenters = {path: detectMarkers(path) for path in imagePaths}\n",
    "\n",
    "# Plot the detected marker points for each image\n",
    "for imagePath, centers in allCenters.items():\n",
    "    image = cv2.imread(imagePath)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image_rgb)\n",
    "\n",
    "    # Overlay detected points\n",
    "    for point in centers:\n",
    "        plt.scatter(point[0], point[1], color='red', s=40)\n",
    "\n",
    "    plt.title(f\"Detected Marker Points for {imagePath}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Print detected points\n",
    "print(\"Detected marker points:\", allCenters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detectMarkers(imagePath):\n",
    "    # Load the image\n",
    "    image = cv2.imread(imagePath)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    \n",
    "    # Thresholding: Detect dark objects (black markers)\n",
    "    _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)  # Invert for black markers\n",
    "    \n",
    "    # Morphological operations: Remove noise\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)  # Remove small noise\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)  # Fill small holes\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    markerCenters = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        # Calculate contour area\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area < 100:  # Ignore very small contours\n",
    "            continue\n",
    "\n",
    "        # Get circularity: (4π * Area) / (Perimeter²)\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        if perimeter == 0:\n",
    "            continue  # Avoid division by zero\n",
    "        \n",
    "        circularity = (4 * np.pi * area) / (perimeter ** 2)\n",
    "        \n",
    "        # Accept only circular objects (close to 1.0)\n",
    "        if 0.7 < circularity <= 1.2:  # Adjust range if needed\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                markerCenters.append((cx, cy))\n",
    "    \n",
    "    return markerCenters\n",
    "\n",
    "# List of image paths\n",
    "imagePaths = [\n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00001.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00002.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00003.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00004.JPG\", \n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00005.JPG\"\n",
    "]\n",
    "\n",
    "# Detect markers for all images\n",
    "allCenters = {path: detectMarkers(path) for path in imagePaths}\n",
    "\n",
    "# Plot the detected marker points for each image\n",
    "for imagePath, centers in allCenters.items():\n",
    "    image = cv2.imread(imagePath)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image_rgb)\n",
    "\n",
    "    # Overlay detected points\n",
    "    for point in centers:\n",
    "        plt.scatter(point[0], point[1], color='red', s=40)\n",
    "\n",
    "    plt.title(f\"Detected Marker Points for {imagePath}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Print detected points\n",
    "print(\"Detected marker points:\", allCenters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above did not use SAM. This uses SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# Load the SAM model\n",
    "model_path = \"/home/ad/22021468/sam_checkpoints/sam_vit_h_4b8939.pth\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=model_path).to(device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "def detectMarkers(imagePath):\n",
    "    \"\"\" Detect markers using Segment Anything Model (SAM). \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(imagePath)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Set image for SAM\n",
    "    predictor.set_image(image_rgb)\n",
    "\n",
    "    # Generate candidate points\n",
    "    height, width, _ = image.shape\n",
    "    input_points = np.array([\n",
    "        [width // 4, height // 4], [3 * width // 4, height // 4], \n",
    "        [width // 4, 3 * height // 4], [3 * width // 4, 3 * height // 4]\n",
    "    ])  # Example points, adjust as needed\n",
    "\n",
    "    input_labels = np.ones(len(input_points))  # Labels for points\n",
    "\n",
    "    # Predict masks\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_points, point_labels=input_labels, multimask_output=True\n",
    "    )\n",
    "\n",
    "    # Extract contours from the mask with the highest score\n",
    "    best_mask = masks[np.argmax(scores)].astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(best_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    markerCenters = []\n",
    "    for contour in contours:\n",
    "        (x, y), radius = cv2.minEnclosingCircle(contour)\n",
    "        if radius > 5:  # Ignore very small detections\n",
    "            markerCenters.append((int(x), int(y)))\n",
    "\n",
    "    return markerCenters\n",
    "\n",
    "# List of image paths\n",
    "imagePaths = [\n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00001.JPG\",\n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00002.JPG\",\n",
    "]\n",
    "\n",
    "# Detect markers for all images\n",
    "allCenters = {path: detectMarkers(path) for path in imagePaths}\n",
    "\n",
    "# Plot the detected marker points for each image\n",
    "for imagePath, centers in allCenters.items():\n",
    "    image = cv2.imread(imagePath)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image_rgb)\n",
    "\n",
    "    for point in centers:\n",
    "        plt.scatter(point[0], point[1], color='red', s=40, label='Detected Marker')\n",
    "\n",
    "    plt.title(f\"Detected Markers in {imagePath}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Detected marker points:\", allCenters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is get the seed points. The above code seems not get the seed points rights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Global variable to store selected points\n",
    "selected_points = []\n",
    "\n",
    "# Mouse callback function to capture clicks\n",
    "def select_points(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        # Append clicked point to the selected_points list\n",
    "        selected_points.append((x, y))\n",
    "        print(f\"Point selected: ({x}, {y})\")\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00001.JPG')\n",
    "\n",
    "# Set up the window for interaction\n",
    "cv2.imshow(\"Select Points\", image)\n",
    "cv2.setMouseCallback(\"Select Points\", select_points)\n",
    "\n",
    "# Wait for the user to click on the image\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Now 'selected_points' will contain all the points that the user clicked on\n",
    "print(\"Selected seed points:\", selected_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an alternative. This is from the internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segment model is loaded first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad/22021468/miniconda3/envs/IndustryExperience/lib/python3.9/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "\n",
    "CHECKPOINT_PATH = \"/home/ad/22021468/sam_checkpoints/sam_vit_h_4b8939.pth\"\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)\n",
    "sam.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated Mask (Instance Segmentation) Generation with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code has loop. It did not have loop in the original example\n",
    "\n",
    "import cv2\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "# Initialize the mask generator\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "# List of image paths\n",
    "image_paths = [\n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00001.JPG\",\n",
    "    \"/data/shared/CSIT_Placement_2025_3D_Reef/CBHE_BA2D_P1/images/frame_00002.JPG\",\n",
    "]\n",
    "\n",
    "# Process each image\n",
    "results = {}\n",
    "for image_path in image_paths:\n",
    "    image_bgr = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Generate segmentation masks\n",
    "    masks = mask_generator.generate(image_rgb)\n",
    "    \n",
    "    # Store results\n",
    "    results[image_path] = masks\n",
    "\n",
    "print(\"Segmentation complete for all images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m detections \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetections\u001b[38;5;241m.\u001b[39mfrom_sam(masks)\n\u001b[1;32m      9\u001b[0m image_bgr \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 10\u001b[0m annotated_images[image_path] \u001b[38;5;241m=\u001b[39m \u001b[43mmask_annotator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_bgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IndustryExperience/lib/python3.9/site-packages/supervision/annotators/core.py:243\u001b[0m, in \u001b[0;36mMaskAnnotator.annotate\u001b[0;34m(self, scene, detections, custom_color_lookup)\u001b[0m\n\u001b[1;32m    240\u001b[0m colored_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scene, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m detection_idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mflip(np\u001b[38;5;241m.\u001b[39margsort(detections\u001b[38;5;241m.\u001b[39marea)):\n\u001b[0;32m--> 243\u001b[0m     color \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_color\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetection_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetection_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_lookup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_lookup\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcustom_color_lookup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcustom_color_lookup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     mask \u001b[38;5;241m=\u001b[39m detections\u001b[38;5;241m.\u001b[39mmask[detection_idx]\n\u001b[1;32m    252\u001b[0m     colored_mask[mask] \u001b[38;5;241m=\u001b[39m color\u001b[38;5;241m.\u001b[39mas_bgr()\n",
      "File \u001b[0;32m~/miniconda3/envs/IndustryExperience/lib/python3.9/site-packages/supervision/annotators/utils.py:83\u001b[0m, in \u001b[0;36mresolve_color\u001b[0;34m(color, detections, detection_idx, color_lookup)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresolve_color\u001b[39m(\n\u001b[1;32m     73\u001b[0m     color: Union[Color, ColorPalette],\n\u001b[1;32m     74\u001b[0m     detections: Detections,\n\u001b[1;32m     75\u001b[0m     detection_idx: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     76\u001b[0m     color_lookup: Union[ColorLookup, np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m ColorLookup\u001b[38;5;241m.\u001b[39mCLASS,\n\u001b[1;32m     77\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Color:\n\u001b[1;32m     78\u001b[0m     idx \u001b[38;5;241m=\u001b[39m resolve_color_idx(\n\u001b[1;32m     79\u001b[0m         detections\u001b[38;5;241m=\u001b[39mdetections,\n\u001b[1;32m     80\u001b[0m         detection_idx\u001b[38;5;241m=\u001b[39mdetection_idx,\n\u001b[1;32m     81\u001b[0m         color_lookup\u001b[38;5;241m=\u001b[39mcolor_lookup,\n\u001b[1;32m     82\u001b[0m     )\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_color_by_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IndustryExperience/lib/python3.9/site-packages/supervision/annotators/utils.py:68\u001b[0m, in \u001b[0;36mget_color_by_index\u001b[0;34m(color, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_color_by_index\u001b[39m(color: Union[Color, ColorPalette], idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Color:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(color, ColorPalette):\n\u001b[0;32m---> 68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcolor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mby_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m color\n",
      "File \u001b[0;32m~/miniconda3/envs/IndustryExperience/lib/python3.9/site-packages/supervision/draw/color.py:390\u001b[0m, in \u001b[0;36mColorPalette.by_idx\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mby_idx\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Color:\n\u001b[1;32m    372\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    Return the color at a given index in the palette.\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midx argument should not be negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    392\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolors)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "import supervision as sv\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(color_lookup = 'CLASS')\n",
    "\n",
    "# Process each image and annotate\n",
    "annotated_images = {}\n",
    "for image_path, masks in results.items():\n",
    "    detections = sv.Detections.from_sam(masks)\n",
    "    image_bgr = cv2.imread(image_path)\n",
    "    annotated_images[image_path] = mask_annotator.annotate(image_bgr, detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections are valid.\n"
     ]
    }
   ],
   "source": [
    "detections = sv.Detections.from_sam(masks)\n",
    "if detections is None or len(detections) == 0:\n",
    "    print(\"No detections found.\")\n",
    "else:\n",
    "    print(\"Detections are valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "image_bgr = cv2.imread(image_path)\n",
    "if image_bgr is None:\n",
    "    print(f\"Failed to load image at {image_path}.\")\n",
    "else:\n",
    "    print(\"Image loaded successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IndustryExperience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
